{"cells":[{"cell_type":"markdown","source":["<h1>Hardware and Software for Big Data Mod. B</h1>\n<h2>Giacomo Matrone</h2>     \n<h3>P37000011</h3>\n<h2>Christian Riccio</h2>    \n<h3>P37000002</h3>\n\n# Abstract\nTwitter è un social media attraverso il quale le persone condividono i loro pensieri e le loro emozioni circa i trend del momento. In un periodo delicato quale quello dell'epidemia da coronavirus che dilaga nel mondo, e che sta toccando tutti personalmente, i social media in generale, e twitter nello specifico, sono più frequentati del solito. La circolazione libera delle informazioni porta inoltre a dover fronteggiare buone, cattive e fake news. In aggiunta a ciò, estrapolare il sentimento generale delle persone circa una problematica importante, quale quella del coronavirus, può aiutare a capire in che direzione si sta muovendo la coscineza di tutti nei confronti del virus.\n\n# Introduzione\nIl lavoro condotto riguarda l'implementazione tecnica di una motore di sentiment analysis, near real-time, utilizzando i tweet degli utenti di Twitter. L'integrazione di Spark e AWS Kinesis, ci ha permesso di condurre l'analisi della serie storica dei tweet riguardanti il coronavirus, filtrandoli per hastag (#COVID-19) e regione di provenienza. L'architettura che abbiamo attualmente usato in tale fase è rappresentata in figura e commentata di seguito:\n\n![img](https://miro.medium.com/max/1400/1*sccUyqc0VR_JBhwNGkfNbg.jpeg)\n\n- E' stata creata un'istanza su AWS EC2 (piano free, visto che siamo **#barboni**), seguendo gli steps indicati qui sotto:\n  - E' stato registrato un account su AWS;\n  - E' stata creata un'istanza EC2;\n  - E' stata creata una chiave SSH tramite RSA;\n  - E' stato usato Putty al fine di poter connettere l'istanza EC2 con il computer di casa;\n  - E' stato usato uno script python contenuto in questo [git](https://github.com/joking-clock/twitter-capture-python) al fine di poter prelevare i tweet da twitter per poi indirizzarli verso Kinesis;\n- E' stata creata un'istanza su AWS Kinesis che preleva i dati tramite uno script python, li renderizza come stream, per poi, infine, trasmetterli su Databricks, dove verranno resi in spark sql.\n  \nInoltre, tecnicamente il processo si esegue nell'ordine definito qui di seguito: \n\n- Il Kinesis Data Stream monitora il flusso di dati fornendoci delle metriche dettagliate sulla corretta ingestione dei dati;\n- Tramite il notebook di Databricks è stata effettuata la connesisone tra Spark Structured Streaming e il flusso di dati proveniente da Kinesis;\n- L'applicazione scritta, ci ha permesso di costruire un motore di sentiment analysis usando: SparkSQL per eseguire query sui dati collezionati in stream e TextBlob (modulo di python) per strutturare la sentiment dei tweet;\n- Infine, tramite il modulo seaborn di python, abbiamo ottenuto un plot del conteggio dei tweet in relazione alla propria sentiment.\n\n\nIn maniera del tutto generale, l'implementazione di un modello di gestione dei Big Data segue il workflow, ben descritto, nell'immagine di seguito:\n![image](https://docs.microsoft.com/it-it/azure/architecture/data-guide/big-data/images/real-time-pipeline.png)\n\nLe architetture per la gestione dei Big Data includono i seguenti componenti:\n\n- Sorgente dati (es. DBs relazionali, dati in tempo reali quali logs di sensori e etc.);\n- Sistema di archiviazione dati;\n- Elaborazione batch: la quantità dimensioni considerevoli, una soluzione per Big Data deve spesso elaborare i file di dati mediante processi batch con esecuzione prolungata per filtrare, aggregare e preparare in altro modo i dati per l'analisi;\n- Inserimento di messaggi in tempo reale: se i dati sono di provenienza real-time, l'architettura deve includere un modo per acquisire e archiviare i messaggi in tempo reale per l'elaborazione del flusso;\n- Elaborazione del flusso: tale fase si conduce dopo avere acquisito i dati in tempo reale, filtrandoli, aggregandoli e preparandoli per l'analisi;\n- Archivio dati analitici: in cui dopo aver preparato i dati con le fasi precedenti si possono elaborare i dati in un formato strutturato, sui quali si possono eseguire query. Numerose soluzioni per Big Data preparano i dati per l'analisi e quindi servono i dati elaborati in un formato strutturato su cui è possibile eseguire query con strumenti analitici;\n- Analisi e creazione di report: si forniscono informazioni dettagliate sui dati tramite strumenti di analisi e report."],"metadata":{}},{"cell_type":"markdown","source":["# Cattura dei tweet\nTwitter fornisce una API per la cattura dei tweet chiama Twitter Labs, con la quale vengono fornite delle credenziali segrete necessarie ad accedere e procedere con il compito preposto. In python il modulo Tweepy da la possibilità di catturare i tweet, real-timr, filtrandoli per key-words. Contestualmente, fornendo le credenziali di AWS ci si collega con Kinesis Data Stream, tramite una macchina remota EC2, che fornisce il necessario per memorizzare, processare e analizzare i dati."],"metadata":{}},{"cell_type":"markdown","source":["## Setup dello streaming e sentiment process\nSi è rovveduto a:\n\n- Definire la spark session;\n- Definire lo spark streaming database;\n- Controllare il processo di data collection;\n- Lanciare query di prova;\n- Visualizzare i dati;\n- Implementare il motore di sentiment analysis;\n- Visualizzare i risultati."],"metadata":{}},{"cell_type":"markdown","source":["## Definiamo la SparkSession\nApache Spark segue l’architettura master slave, dove il master è rappresentato dal driver e gli slaves dagli esecutori. Le applicazioni Spark vengono eseguite come serie indipendenti di processi su un cluster, coordinate dall’oggetto SparkContext o SparkSession nel programma principale (chiamato programma del driver). In particolare, per essere eseguito su un cluster, SparkSession può connettersi a diversi tipi di gestori di cluster o Cluster Manager (es.YARN), che allocano risorse tra le applicazioni. Una volta connesso, Spark acquisisce gli esecutori sui nodi del cluster, che sono processi che eseguono calcoli e archiviano i dati per l’applicazione. Successivamente, invia il codice dell’applicazione agli esecutori. Infine, SparkSession invia le attività agli esecutori per l’esecuzione. Gli esecutori sono responsabili dell’esecuzione effettiva del lavoro assegnatogli dal driver. Ciò significa che ogni esecutore è responsabile solo di due cose: eseguire il codice assegnatogli dal driver e riportare lo stato del calcolo, su quell’esecutore, al nodo del driver. Questa modalità di esecuzione, in cui il driver è uno degli esecutori nel cluster, viene detta Cluster Mode. Procediamo quindi all'import dei pacchetti necessari."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split\n\nspark = SparkSession.builder\\\n                    .master(\"local\")\\\n                    .appName(\"Structured Streaming\")\\\n                    .getOrCreate()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["L'entità centrale di Spark è **l'RDD** (Resilient Distribuited Dataset). L'RDD è una collezione di elementi partizionata tra i nodi computazionali (nel caso di Databricks Community Edition sono disponibili 2 nodi computazionali), ai fini di **introdurre un parallelismo**. L'architettura Master-Slave, unitamente al concetto di RDD, hanno reso Spark a tutti gli effetti fault-tollerant."],"metadata":{}},{"cell_type":"markdown","source":["## Da Kinesis Data Stream a Spark Streaming DataFrame\n\nTutte le funzionalità di Apache Spark dipendono da Spark Core. Poggiati su Spark Core, troviamo:\n\n- Spark Streaming: che consente l’elaborazione di flussi live di dati real time\n- SparkSQL: che consente di lavorare con dati strutturati, eseguendo query sugli stessi. Di seguito è riportato il codice per definire la struttura dello stream di dati:"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructType, StringType\npythonSchema = StructType() \\\n          .add(\"id\", StringType(), True) \\\n          .add(\"tweet\", StringType(), True) \\\n          .add(\"ts\", StringType(), True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"markdown","source":["Creiamo ora il flusso di dati da **kines a Spark Streaming** fornendo alcuni parametri di collegamento, unitamente allo schema della struttura dati di cui sopra:"],"metadata":{}},{"cell_type":"code","source":["awsAccessKeyId = \"****************\" # valore access key\nawsSecretKey = \"********************\"   # valore secret key\nkinesisStreamName = \"***********\"  # nome kinesis stream\nkinesisRegion = \"***********\"\nkinesisDF = spark \\\n  .readStream \\\n  .format(\"kinesis\") \\\n  .option(\"streamName\", kinesisStreamName)\\\n  .option(\"region\", kinesisRegion) \\\n  .option(\"initialPosition\", \"LATEST\") \\\n  .option(\"format\", \"json\") \\\n  .option(\"awsAccessKey\", awsAccessKeyId)\\\n  .option(\"awsSecretKey\", awsSecretKey) \\\n  .option(\"inferSchema\", \"true\") \\\n  .load()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"markdown","source":["## Injection dei dati\n\nCon lo script python per la cattura dei tweet in esecuzione sulla Virtual Machine e su Kinesis Data Stream di AWS, possiamo vedere come i dati provenienti da Kinesis, entrano in Spark Streaming:"],"metadata":{}},{"cell_type":"code","source":["df = kinesisDF \\\n  .writeStream \\\n  .format(\"memory\") \\\n  .outputMode(\"append\") \\\n  .queryName(\"tweets\")  \\\n  .start()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["`df` ci permette di eseguire query sullo streming dati e con `.format('memory')` ci assicuriamo che l'output del flusso sia salvato in memoria come tabella avente lo schema definito sopra. Per sapere se i dati sono disponibili e di conseguenza iniziare ad esplorarli, si è proceduto come di seguito, eseguendo anche una prima query:"],"metadata":{}},{"cell_type":"code","source":["df.status"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[15]: {&#39;message&#39;: &#39;Stopped&#39;, &#39;isDataAvailable&#39;: False, &#39;isTriggerActive&#39;: False}</div>"]}}],"execution_count":14},{"cell_type":"markdown","source":["## Data Visualization"],"metadata":{}},{"cell_type":"code","source":["import pandas as pd"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["tweets = spark.sql(\"select cast(data as string) from tweets\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":17},{"cell_type":"markdown","source":["## Visualizziamo e valutiamo i dati:"],"metadata":{}},{"cell_type":"code","source":["tweets.show(5,truncate=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n                data|\n+--------------------+\n[{&#34;id&#34;: &#34;12692238...|\n[{&#34;id&#34;: &#34;12692238...|\n[{&#34;id&#34;: &#34;12692238...|\n[{&#34;id&#34;: &#34;12692238...|\n[{&#34;id&#34;: &#34;12692238...|\n+--------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":19},{"cell_type":"markdown","source":["### User defined function\n\nAl fine di poter caricare un dataframe pandas, usiamo una user defined funcion che agisca sui tweet collezionati a livello di stream. Questo ci consente di creare un datamodel più regolare, al fine, poi, di caricare il tutto in un formato correttamente interpretabile da Pandas."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nimport json\nfrom pyspark.sql.functions import UserDefinedFunction\ndef parse_tweet(text):\n    data = json.loads(text)\n    id = data[0]['id']\n    ts = data[0]['ts']\n    tweet = data[0]['tweet'] \n    return (id, ts, tweet)\n    \n# Define your function\ngetID = UserDefinedFunction(lambda x: parse_tweet(x)[0], StringType())\ngetTs = UserDefinedFunction(lambda x: parse_tweet(x)[1], StringType())\ngetTweet = UserDefinedFunction(lambda x: parse_tweet(x)[2], StringType())\n# Apply the UDF using withColumn\ntweets = (tweets.withColumn('id', getID(col(\"data\")))\n               .withColumn('ts', getTs(col(\"data\")))\n               .withColumn('tweet', getTweet(col(\"data\")))\n         )"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["tweets.show(5,truncate=True)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-------------------+--------------------+--------------------+\n                data|                 id|                  ts|               tweet|\n+--------------------+-------------------+--------------------+--------------------+\n[{&#34;id&#34;: &#34;12692238...|1269223863780589568|Sat Jun 06 11:05:...|b&#39;RT @WHO: Media ...|\n[{&#34;id&#34;: &#34;12692238...|1269223864984383488|Sat Jun 06 11:05:...|b&#39;Cappuccino e co...|\n[{&#34;id&#34;: &#34;12692238...|1269223865630236679|Sat Jun 06 11:05:...|b&#39;RT @WHO: WHO up...|\n[{&#34;id&#34;: &#34;12692238...|1269223867274399744|Sat Jun 06 11:05:...|b&#39;RT @BerndPulver...|\n[{&#34;id&#34;: &#34;12692238...|1269223867484119041|Sat Jun 06 11:05:...|b&#39;RT @BeachMilk: ...|\n+--------------------+-------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["!pip install textblob"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"markdown","source":["## Sentiment\n\nLa sentiment analisi e parte del Natural Language Processing e che ha los scopo di classificare e interpretare il sentiment di un trend dal testo. Il modulo textblob di python fornisce di base un modello precaricato necessario per ottemperare al nostro goal. L'idea di base è quella di classificare la polarità di un dato tweet in:\n\n- neutrale;\n- positivo;\n- negativo.\n\nattribuendo a ciascuno di essi un valore compreso tra [-1,1]"],"metadata":{}},{"cell_type":"code","source":["import textblob\ndef get_sentiment(text):\n    from textblob import TextBlob\n    tweet = TextBlob(text)\n    if tweet.sentiment.polarity < 0:\n      sentiment = \"negative\"\n    elif tweet.sentiment.polarity == 0:\n        sentiment = \"neutral\"\n    else:\n        sentiment = \"positive\"\n    return sentiment"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":25},{"cell_type":"code","source":["pandas_tweets = tweets.toPandas()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":26},{"cell_type":"markdown","source":["Tramite il metodo `map` di pandas series, possiamo poi ottenere una nuova colonna che ci dia il sentiment rilevato in relazione ai singoli threshold definiti sopra."],"metadata":{}},{"cell_type":"code","source":["pandas_tweets['polarity'] = pandas_tweets.tweet.map(lambda x: get_sentiment(x))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":28},{"cell_type":"code","source":["pandas_tweets.head()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>data</th>\n      <th>id</th>\n      <th>ts</th>\n      <th>tweet</th>\n      <th>polarity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[{\"id\": \"1269223863780589568\", \"tweet\": \"b'RT ...</td>\n      <td>1269223863780589568</td>\n      <td>Sat Jun 06 11:05:24 +0000 2020</td>\n      <td>b'RT @WHO: Media briefing on #COVID19 with @Dr...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[{\"id\": \"1269223864984383488\", \"tweet\": \"b'Cap...</td>\n      <td>1269223864984383488</td>\n      <td>Sat Jun 06 11:05:25 +0000 2020</td>\n      <td>b'Cappuccino e cornetto? Cappuccino e brioche?...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[{\"id\": \"1269223865630236679\", \"tweet\": \"b'RT ...</td>\n      <td>1269223865630236679</td>\n      <td>Sat Jun 06 11:05:25 +0000 2020</td>\n      <td>b'RT @WHO: WHO updated guidance on the use of ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[{\"id\": \"1269223867274399744\", \"tweet\": \"b'RT ...</td>\n      <td>1269223867274399744</td>\n      <td>Sat Jun 06 11:05:25 +0000 2020</td>\n      <td>b'RT @BerndPulverer: Calling on @TheLancet &amp;am...</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[{\"id\": \"1269223867484119041\", \"tweet\": \"b'RT ...</td>\n      <td>1269223867484119041</td>\n      <td>Sat Jun 06 11:05:25 +0000 2020</td>\n      <td>b'RT @BeachMilk: Trump mentioned U.V. light \\x...</td>\n      <td>positive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["import seaborn as sns\nimport matplotlib.pyplot as plt"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"markdown","source":["### Visualizzazione del sentiment\n\nAl fine di poter avere un'idea del sentiment medio, andiamo poi a creare un grafico a barre in relazione al sentiment rilevato nei tweet. Una possibile applicazione di questo risultato consisterebbe nel poter studiare, in un secondo momento, l'andamento della sentiment durante i periodi di lock down."],"metadata":{}},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(10,10))\nsns.countplot(pandas_tweets.polarity,ax = ax)\nplt.title(\"Tweets sentiment at %s\"%(pandas_tweets.ts.max()))\nplt.show()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["! pip install TwitterAPI"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":33},{"cell_type":"markdown","source":["## Batch analysis\n\nPer analizzare l'andamento dei tweet nel tempo, prepariamo una batch analysis con i dati di twitter storici, presi in quattro giorni consecutivi nei mesi di Febbraio, Marzo ed Aprile.\nL'analisi mostra l'andamento nel tempo del sentiment notato."],"metadata":{}},{"cell_type":"code","source":["from TwitterAPI import TwitterAPI, TwitterPager\nimport csv\n# SEARCH_TERM = 'pizza'\n\n# api = TwitterAPI(<consumer key>, \n#                  <consumer secret>,\n#                  <access token key>,\n#                  <access token secret>)\n\n# pager = TwitterPager(api, 'search/tweets', {'q': SEARCH_TERM})\n\n# for item in pager.get_iterator():\n#     print(item['text'] if 'text' in item else item)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":35},{"cell_type":"code","source":["import csv\nimport pandas as pd\nimport sys\nfrom textblob import TextBlob\nimport re\nimport datetime\n####input your credentials here\nconsumer_key = '76gdbmQlVhj0JZE4UpoVzovJ1'\nconsumer_secret = 'YfPKo66ptcI96d1xU1rpXJT8cYMRpCcYFZQxBq9cudCPcHMUMo'\naccess_token = '1218643738026815488-zgfH6FM4w4DlxZdq9Uazvu3hCq4hNf'\naccess_token_secret = '5wwuolifLJrJm6EqZSJtyIT45HD8iUDF4HNc72FUzR4HC'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":36},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import split \n\nspark = SparkSession.builder\\\n                    .master(\"local\")\\\n                    .appName(\"Structured Streaming\")\\\n                    .getOrCreate()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":37},{"cell_type":"code","source":["SEARCH_TERM = ['#coronavirus']\nPRODUCT = 'fullarchive'\nLABEL = 'DBMS2'\n\napi = TwitterAPI(consumer_key, \n             consumer_secret, \n             access_token, \n             access_token_secret)\nhist_tweets=[]\nfor i in [\"202002220000\",\"202002230000\",\"202002240000\",\"202002250000\",\"202002260000\",\"202003220000\",\"202003230000\",\"202003240000\",\"202003250000\",\"202003260000\",\"202004220000\",\"202004230000\",\"202004240000\",\"202004250000\",\"202004260000\"]:\n  r = api.request('tweets/search/%s/:%s' % (PRODUCT, LABEL), \n              {'query':SEARCH_TERM,\n              'toDate':i,\n              }\n              )\n  if r.status_code != 200:\n    raise Exception(\"error on API %s\"%(r.status_code))\n\n  for item in r:\n\n    payload={'id':str(item['id']),'tweet':str(item['text']),'created_at':str(item['created_at'])}\n    if 'next' not in json:\n        break\n    hist_tweets.append(payload)\n    next = json['next']\n  "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Exception</span>                                 Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2831441279555967&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     15</span>               )\n<span class=\"ansi-green-intense-fg ansi-bold\">     16</span>   <span class=\"ansi-green-fg\">if</span> r<span class=\"ansi-blue-fg\">.</span>status_code <span class=\"ansi-blue-fg\">!=</span> <span class=\"ansi-cyan-fg\">200</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">---&gt; 17</span><span class=\"ansi-red-fg\">     </span><span class=\"ansi-green-fg\">raise</span> Exception<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;error on API %s&#34;</span><span class=\"ansi-blue-fg\">%</span><span class=\"ansi-blue-fg\">(</span>r<span class=\"ansi-blue-fg\">.</span>status_code<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">     18</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">     19</span>   <span class=\"ansi-green-fg\">for</span> item <span class=\"ansi-green-fg\">in</span> r<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-red-fg\">Exception</span>: error on API 429</div>"]}}],"execution_count":38},{"cell_type":"code","source":["hist_data = pd.DataFrame(hist_tweets)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":39},{"cell_type":"code","source":[""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>created_at</th><th>id</th><th>tweet</th><th>polarity</th></tr></thead><tbody><tr><td>Wed Mar 25 23:59:54 +0000 2020</td><td>1242964458361716737</td><td>Está partiendo desde Miami un avión de @Helidosa con destino a China que traerá la ayuda anunciada para combatir el… https://t.co/FOYoEl7nJ6</td><td>neutral</td></tr><tr><td>Wed Mar 25 23:59:54 +0000 2020</td><td>1242964457854287872</td><td>RT @fatourgente: Governo dos Estados Unidos orienta que seus cidadãos deixem o Brasil imediatamente #coronavírus https://t.co/thYkPHWHsn</td><td>neutral</td></tr><tr><td>Wed Mar 25 23:59:54 +0000 2020</td><td>1242964457787207681</td><td>RT @SenTedCruz: The Chinese Communist Party did everything it could to keep the origin &amp; spread of #coronavirus a secret. Now that thousand…</td><td>negative</td></tr><tr><td>Wed Mar 25 23:59:54 +0000 2020</td><td>1242964457611018241</td><td>#Creatividad en caricaturas. [📹]\n\nEsta es la forma más divertida para hacerle frente a la contingencia mundial del… https://t.co/8t42I1GOgn</td><td>neutral</td></tr><tr><td>Wed Mar 25 23:59:54 +0000 2020</td><td>1242964457292267521</td><td>RT @arepandro: Cuando salgan las cachiporreras del Gobierno corrupto de Moreno @ramirogarciaf @martharoldos etc a querer endilgar la inoper…</td><td>neutral</td></tr></tbody></table></div>"]}}],"execution_count":40},{"cell_type":"code","source":["hist_data['polarity'] = hist_data.tweet.map(lambda x: get_sentiment(x))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":41},{"cell_type":"code","source":["feb_data = hist_data[:500]\nmarch_data = hist_data[500:1000]\napril_data = hist_data[1000:]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":42},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(10,10))\nsns.countplot(feb_data.polarity,ax = ax)\nplt.title(\"Tweets sentiment at February\")\nplt.show()"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(10,10))\nsns.countplot(march_data.polarity,ax = ax)\nplt.title(\"Tweets sentiment at March\")\nplt.show()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(10,10))\nsns.countplot(april_data.polarity,ax = ax)\nplt.title(\"Tweets sentiment at April\")\nplt.show()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["# Conclusioni\nLa costruzione di un motore di near real-time sentiment analysis dei tweet degli utenti di Twitter ci ha permesso di dedurre che la magigor parte dei twet postati presenta un sentimento neutrale nei conforni del virus e dell'andamento dello stesso in generale nel tempo, portando alla luce aspetti che potrebbero interessare la salute pubblica. In particolare un sentiment neutrale ci può far considerare che le persone non sembrano essere state interessate da quanto ci sta toccando. Non di meno, questo lavoro ha voluto rappresentare una prima mesa in pratica di una pipeline di gestione e analisi di un sistema di Big Data, combinando le potenzialità offerte dai servizi in cloud di Amazon AWS e l'elasticità garantita da Apache Spark in fatto di elaborazione e analisi su un grande volume di dati, quale nel caso in easame, i tweet di una fetta di utenti di Twitter. Ovviamente, le considerazioni sino ad ora portate, sono relative e risentono dell'esiguità del campione considerato, per vincoli del piano free di twitter. Il sentiment appare comunque altamente altalenante."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":47}],"metadata":{"name":"HW-SW-BIG-DATA","notebookId":2115824784709785},"nbformat":4,"nbformat_minor":0}
